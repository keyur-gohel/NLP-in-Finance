{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP.ipynb","provenance":[{"file_id":"1ril39NYHhb-FFAg_qqIUTzyaz0A9vYnz","timestamp":1628940431646}],"collapsed_sections":[],"authorship_tag":"ABX9TyMFPyIxILFTA72FJDT+yR22"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sVxz9sDw9e-B"},"source":["## **Dow Jones Industrial Average (DJIA) Movement Prediction Using News Headlines**\n","\n","\n","\n","> We have taken a dataset of top 25 news headlines for a day and combine it with Dow Jones Industrial Average (DJIA) index movement where, \"0\" refers to the downward movement and \"1\" referes to the upwoed movement. We created a classifier to predict DJIA index movement based on the given headlines. Data covers the span of 2008-2016. \n","\n"]},{"cell_type":"code","metadata":{"id":"au0zK6g85R3X"},"source":["# Import Libraries\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from collections import defaultdict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhjf65jSSQHD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629017193376,"user_tz":-330,"elapsed":1386,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"dbd22a6a-9d1c-4956-ed02-fce94fbd5513"},"source":["# Importing libraries (NLTK)\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet as wn\n","\n","# Note:- There are some additional functions used which need to be downloded for first time. To download uncomment the code below.\n","\n","# nltk.download('punkt')\n","# nltk.download('wordnet')\n","# nltk.download('averaged_perceptron_tagger')\n","# nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"k7AM7joI10gv"},"source":["# Import Libraries (model creation and vectorizer)\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn import svm\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"LnJ4xAvz9dz4","executionInfo":{"status":"ok","timestamp":1629017302089,"user_tz":-330,"elapsed":395,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"27e36c37-9b1c-44b3-b11a-c386920e6c20"},"source":["# Read in the data\n","np.random.seed(500)\n","data = pd.read_csv('Full_Data.csv', encoding = \"ISO-8859-1\")\n","data.head(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Label</th>\n","      <th>Top1</th>\n","      <th>Top2</th>\n","      <th>Top3</th>\n","      <th>Top4</th>\n","      <th>Top5</th>\n","      <th>Top6</th>\n","      <th>Top7</th>\n","      <th>Top8</th>\n","      <th>Top9</th>\n","      <th>Top10</th>\n","      <th>Top11</th>\n","      <th>Top12</th>\n","      <th>Top13</th>\n","      <th>Top14</th>\n","      <th>Top15</th>\n","      <th>Top16</th>\n","      <th>Top17</th>\n","      <th>Top18</th>\n","      <th>Top19</th>\n","      <th>Top20</th>\n","      <th>Top21</th>\n","      <th>Top22</th>\n","      <th>Top23</th>\n","      <th>Top24</th>\n","      <th>Top25</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-01-03</td>\n","      <td>0</td>\n","      <td>A 'hindrance to operations': extracts from the...</td>\n","      <td>Scorecard</td>\n","      <td>Hughes' instant hit buoys Blues</td>\n","      <td>Jack gets his skates on at ice-cold Alex</td>\n","      <td>Chaos as Maracana builds up for United</td>\n","      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n","      <td>Hungry Spurs sense rich pickings</td>\n","      <td>Gunners so wide of an easy target</td>\n","      <td>Derby raise a glass to Strupar's debut double</td>\n","      <td>Southgate strikes, Leeds pay the penalty</td>\n","      <td>Hammers hand Robson a youthful lesson</td>\n","      <td>Saints party like it's 1999</td>\n","      <td>Wear wolves have turned into lambs</td>\n","      <td>Stump mike catches testy Gough's taunt</td>\n","      <td>Langer escapes to hit 167</td>\n","      <td>Flintoff injury piles on woe for England</td>\n","      <td>Hunters threaten Jospin with new battle of the...</td>\n","      <td>Kohl's successor drawn into scandal</td>\n","      <td>The difference between men and women</td>\n","      <td>Sara Denver, nurse turned solicitor</td>\n","      <td>Diana's landmine crusade put Tories in a panic</td>\n","      <td>Yeltsin's resignation caught opposition flat-f...</td>\n","      <td>Russian roulette</td>\n","      <td>Sold out</td>\n","      <td>Recovering a title</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Date  Label  ...     Top24               Top25\n","0  2000-01-03      0  ...  Sold out  Recovering a title\n","\n","[1 rows x 27 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"_6PZuVB0_-qQ","executionInfo":{"status":"ok","timestamp":1629017303701,"user_tz":-330,"elapsed":62,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"522ffcd5-272b-4047-b321-64ba1bbba5a7"},"source":["# Combine News Headlines into one for sementic analysis\n","data['headlines'] = data[data.columns[2:]].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n","\n","# Creating copy of data (corpus)\n","corpus = data.copy()\n","corpus = corpus[['Date', 'headlines', 'Label']]\n","corpus.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>headlines</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-01-03</td>\n","      <td>A 'hindrance to operations': extracts from the...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000-01-04</td>\n","      <td>Scorecard The best lake scene Leader: German s...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-01-05</td>\n","      <td>Coventry caught on counter by Flo United's riv...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-01-06</td>\n","      <td>Pilgrim knows how to progress Thatcher facing ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-01-07</td>\n","      <td>Hitches and Horlocks Beckham off but United su...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Date                                          headlines  Label\n","0  2000-01-03  A 'hindrance to operations': extracts from the...      0\n","1  2000-01-04  Scorecard The best lake scene Leader: German s...      0\n","2  2000-01-05  Coventry caught on counter by Flo United's riv...      0\n","3  2000-01-06  Pilgrim knows how to progress Thatcher facing ...      1\n","4  2000-01-07  Hitches and Horlocks Beckham off but United su...      1"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4rbrdEVA29t","executionInfo":{"status":"ok","timestamp":1629017304095,"user_tz":-330,"elapsed":9,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"df4ef88d-67d3-4ab7-b568-9cd3230e13fe"},"source":["# SHape of data\n","corpus.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4101, 3)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WW50no8_Q_Mt","executionInfo":{"status":"ok","timestamp":1629017304648,"user_tz":-330,"elapsed":9,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"e5f58572-4d79-43f7-c484-fd94ee759216"},"source":["corpus.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Date         0\n","headlines    0\n","Label        0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"pV0s_xWEP6an","executionInfo":{"status":"ok","timestamp":1629017311012,"user_tz":-330,"elapsed":5748,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"4d6869f0-64d9-4326-853e-661ddeaf28cb"},"source":["# Convert every words of headline to lowercase to eliminate the diferrence of uppercase and lowercase words \n","corpus['headlines'] = [entry.lower() for entry in corpus['headlines']]\n","\n","# Tokenizing headlines\n","corpus['headlines'] = [word_tokenize(entry) for entry in corpus['headlines']]\n","corpus.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>headlines</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-01-03</td>\n","      <td>[a, 'hindrance, to, operations, ', :, extracts...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000-01-04</td>\n","      <td>[scorecard, the, best, lake, scene, leader, :,...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-01-05</td>\n","      <td>[coventry, caught, on, counter, by, flo, unite...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-01-06</td>\n","      <td>[pilgrim, knows, how, to, progress, thatcher, ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-01-07</td>\n","      <td>[hitches, and, horlocks, beckham, off, but, un...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Date                                          headlines  Label\n","0  2000-01-03  [a, 'hindrance, to, operations, ', :, extracts...      0\n","1  2000-01-04  [scorecard, the, best, lake, scene, leader, :,...      0\n","2  2000-01-05  [coventry, caught, on, counter, by, flo, unite...      0\n","3  2000-01-06  [pilgrim, knows, how, to, progress, thatcher, ...      1\n","4  2000-01-07  [hitches, and, horlocks, beckham, off, but, un...      1"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"KoEw32oUR1n0","executionInfo":{"status":"ok","timestamp":1629017515716,"user_tz":-330,"elapsed":114041,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"1c184828-9c73-4958-9c28-a917de60b259"},"source":["# Remove Stop words, Numerics and perfom Word Stemming/Lemmenting.\n","\n","# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective\n","tag_map = defaultdict(lambda : wn.NOUN)\n","tag_map['J'] = wn.ADJ\n","tag_map['V'] = wn.VERB\n","tag_map['R'] = wn.ADV\n","tag_map\n","\n","for index,entry in enumerate(corpus['headlines']):\n","    \n","    # Empty List to store the words for each headline\n","    Final_words = []\n","\n","    # Initializing WordNetLemmatizer()\n","    word_Lemmatized = WordNetLemmatizer()\n","    \n","    # Function pos_tag will provide the 'tag' i.e if the word is Noun(N) or Verb\n","    for word, tag in pos_tag(entry):\n","\n","        # Condition is to check for Stop words and consider only alphabets\n","        if word not in stopwords.words('english') and word.isalpha():\n","            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n","            Final_words.append(word_Final)\n","\n","    # The final processed set of words for each iteration will be stored in 'headlines'\n","    corpus.loc[index,'headlines'] = str(Final_words)\n","    \n","corpus.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>headlines</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2000-01-03</td>\n","      <td>['operation', 'extract', 'leaked', 'report', '...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2000-01-04</td>\n","      <td>['scorecard', 'best', 'lake', 'scene', 'leader...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-01-05</td>\n","      <td>['coventry', 'catch', 'counter', 'flo', 'unite...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-01-06</td>\n","      <td>['pilgrim', 'know', 'progress', 'thatcher', 'f...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-01-07</td>\n","      <td>['hitch', 'horlocks', 'beckham', 'united', 'su...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Date                                          headlines  Label\n","0  2000-01-03  ['operation', 'extract', 'leaked', 'report', '...      0\n","1  2000-01-04  ['scorecard', 'best', 'lake', 'scene', 'leader...      0\n","2  2000-01-05  ['coventry', 'catch', 'counter', 'flo', 'unite...      0\n","3  2000-01-06  ['pilgrim', 'know', 'progress', 'thatcher', 'f...      1\n","4  2000-01-07  ['hitch', 'horlocks', 'beckham', 'united', 'su...      1"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"wevPEzeXVCR7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629017515719,"user_tz":-330,"elapsed":15,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"b4c4f01f-0606-4f54-a2eb-9dbbe0e01c7e"},"source":["# Train Test split\n","# We have data as time series so, it's not a good approach to split train and test split randomly. \n","# We used data from 2000-2014 for training and rest as testing.\n","\n","train = corpus[corpus['Date'] < '20150101']\n","test = corpus[corpus['Date'] > '20141231']\n","\n","Train_X = train['headlines'].values\n","Train_Y = train['Label'].values\n","\n","Test_X = test['headlines'].values\n","Test_Y = test['Label'].values\n","\n","print(Train_X.shape)\n","print(Test_X.shape)\n","print(Train_Y.shape)\n","print(Test_Y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3975,)\n","(378,)\n","(3975,)\n","(378,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4CikrZurt-MT"},"source":["# Convert words to vectors\n","\n","# Function to generate TF-IDF vectors (features) for headlines\n","def tfidf_vectorizer(train_X, test_X, ng_range):\n","  \n","  \"\"\"\n","  params: \n","  train_X: numpy array consisting training data (X = headlines)\n","  test_X: numpy array consisting test data (Y = label)\n","  ng_range: tuple having minimum and maximum range for NGram \n","  \"\"\"\n","\n","  Tfidf_vect = TfidfVectorizer(ngram_range=ng_range)\n","  Tfidf_vect.fit(train_X)\n","  Train_X_Tfidf = Tfidf_vect.transform(train_X)\n","  Test_X_Tfidf = Tfidf_vect.transform(test_X)\n","\n","  # Display the shape of training and test set after tf-idf vectorizer  \n","  print(Train_X_Tfidf.shape)\n","  print(Test_X_Tfidf.shape)\n","\n","  return Train_X_Tfidf, Test_X_Tfidf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i9z5Mrc5ZONz"},"source":["# Model Creation and Accuracy \n","# We are using SVM for classification due to it's batter performance over logistic regression, K-nn, and Trees \n","# for text data. (later we have tried with Multinomial Naive Bayes which tend to perform slightly well)\n","\n","def svm_model(Train_X, Test_X, Train_Y, Test_Y):\n","  \n","  # Classifier - Algorithm - SVM\n","  # fit the training dataset on the classifier\n","  SVM = svm.SVC(C=1, kernel='linear', class_weight='balanced', gamma='auto')\n","  SVM.fit(Train_X, Train_Y)\n","\n","  # predict the labels on validation dataset\n","  predictions_SVM = SVM.predict(Test_X)\n","  # Use accuracy_score function to get the accuracy\n","  accuracy = accuracy_score(Test_Y, predictions_SVM)*100\n","\n","  # Display accuracy just to check\n","  print(\"SVM Accuracy Score -> \",accuracy)\n","\n","  return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyGA_hO52Lje","executionInfo":{"status":"ok","timestamp":1629018922306,"user_tz":-330,"elapsed":236753,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"72b6ef2b-42f7-486b-fa19-bbd9121fcd30"},"source":["# Check for which NGram range model gives best performance.\n","\n","result = {}\n","for i in range(1, 4):\n","  for j in range(i, 4):\n","    r = (i, j)\n","    print(\"============= \", r, \" ==============\")\n","    train_X_tfidf, test_X_tfidf = tfidf_vectorizer(Train_X, Test_X, ng_range=r)\n","    result[r] = svm_model(train_X_tfidf, test_X_tfidf, Train_Y, Test_Y)\n","\n","result"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=============  (1, 1)  ==============\n","(3975, 36724)\n","(378, 36724)\n","SVM Accuracy Score ->  79.8941798941799\n","=============  (1, 2)  ==============\n","(3975, 570795)\n","(378, 570795)\n","SVM Accuracy Score ->  84.12698412698413\n","=============  (1, 3)  ==============\n","(3975, 1276656)\n","(378, 1276656)\n","SVM Accuracy Score ->  84.92063492063492\n","=============  (2, 2)  ==============\n","(3975, 534071)\n","(378, 534071)\n","SVM Accuracy Score ->  83.86243386243386\n","=============  (2, 3)  ==============\n","(3975, 1239932)\n","(378, 1239932)\n","SVM Accuracy Score ->  84.39153439153439\n","=============  (3, 3)  ==============\n","(3975, 705861)\n","(378, 705861)\n","SVM Accuracy Score ->  81.74603174603175\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{(1, 1): 79.8941798941799,\n"," (1, 2): 84.12698412698413,\n"," (1, 3): 84.92063492063492,\n"," (2, 2): 83.86243386243386,\n"," (2, 3): 84.39153439153439,\n"," (3, 3): 81.74603174603175}"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CnIg61JuGfbb","executionInfo":{"status":"ok","timestamp":1629025865455,"user_tz":-330,"elapsed":6630509,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"98e64ac4-e01b-4f42-f74c-3e9839a2c6f1"},"source":["Train_X_Tfidf, Test_X_Tfidf = tfidf_vectorizer(Train_X, Test_X, (2, 2))\n","\n","# Hyperparameter tuning with Grid Search for SVM\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import ParameterGrid\n","param_grid = [{'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [0.1, 1, 10, 100], \n","                                       'gamma': [0.0001, 0.1, 1, 3]}]\n","print(\"Total length to check for best model and parameters: \",len(list(ParameterGrid(param_grid))))\n","gsc = GridSearchCV(svm.SVC(), param_grid, scoring='accuracy')\n","grid_result = gsc.fit(Train_X_Tfidf,Train_Y)\n","print(\"Best Parameters: \", grid_result.best_params_)\n","print(\"Best Estimator: \", grid_result.best_estimator_)\n","print(\"Best Score: \", grid_result.best_score_)\n","\n","y_pred = grid_result.predict(Test_X_Tfidf)\n","print(\"Accuracy:\", accuracy_score(Test_Y, y_pred))\n","# # results['rbf'] = accuracy_score(y_test, y_pred)\n","# print(\"Classification Report: \")\n","# print(classification_report(Test_Y, y_pred))\n","# print(\"Confusion Matrix: \")\n","# print(confusion_matrix(Test_Y, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3975, 534071)\n","(378, 534071)\n","Total length to check for best model and parameters:  64\n","Best Parameters:  {'C': 0.1, 'gamma': 0.0001, 'kernel': 'linear'}\n","Best Estimator:  SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='linear',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False)\n","Best Score:  0.5272955974842767\n","Accuracy: 0.5079365079365079\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TheldofY3CKE","executionInfo":{"status":"ok","timestamp":1629019024100,"user_tz":-330,"elapsed":4320,"user":{"displayName":"keyur gohel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik1iujx61GBvD6alCAYQFeUfUVgxOXXq7qUPFn=s64","userId":"03273641675408672995"}},"outputId":"997b894d-4342-47ca-aa3f-f7b9ec3f54af"},"source":["# Multinomial Naive Bayes for NGram range (2, 2)\n","\n","Train_X_Tfidf, Test_X_Tfidf = tfidf_vectorizer(Train_X, Test_X, (2, 2))\n","from sklearn.naive_bayes import MultinomialNB\n","\n","mnb = MultinomialNB()\n","mnb.fit(Train_X_Tfidf, Train_Y)\n","\n","pred = mnb.predict(Test_X_Tfidf)\n","acc = accuracy_score(Test_Y, pred)*100\n","print(acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3975, 534071)\n","(378, 534071)\n","85.44973544973546\n"],"name":"stdout"}]}]}